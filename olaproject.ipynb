{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV8kpGE7kJOW"
      },
      "outputs": [],
      "source": [
        "#1. Import Libraries and Load Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/492/original/ola_driver_scaler.csv')\n",
        "\n",
        "# Check dataset structure\n",
        "print(df.head())\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns\n",
        "df.rename(columns={\n",
        "    'MMM-YY': 'Reporting Date',\n",
        "    'Dateofjoining': 'Date of Joining',\n",
        "    'LastWorkingDate': 'Last Working Date'\n",
        "}, inplace=True)\n",
        "\n",
        "# Convert the columns to datetime\n",
        "df['Reporting Date'] = pd.to_datetime(df['Reporting Date'], format='%d/%m/%y')\n",
        "df['Date of Joining'] = pd.to_datetime(df['Date of Joining'], format='%d/%m/%y')\n",
        "df['Last Working Date'] = pd.to_datetime(df['Last Working Date'], format='%d/%m/%y')\n",
        "\n"
      ],
      "metadata": {
        "id": "_IL01e2Yp63p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating the data by Driver_ID\n",
        "df_agg = df.groupby('Driver_ID').agg({\n",
        "    'Age': 'max',\n",
        "    'Gender': 'max',\n",
        "    'City': 'first',\n",
        "    'Education_Level': 'first',\n",
        "    'Income': 'mean',\n",
        "    'Date of Joining': 'first',\n",
        "    'Last Working Date': 'first',\n",
        "    'Joining Designation': 'first',\n",
        "    'Grade': 'mean',\n",
        "    'Total Business Value': 'sum',\n",
        "    'Quarterly Rating': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Check the aggregated DataFrame\n",
        "print(df_agg.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "6jY-rRCsPsCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: KNN Imputation\n",
        "# First, create a subset of numerical columns for KNN imputation\n",
        "numerical_columns = ['Age', 'Income', 'Total Business Value', 'Quarterly Rating']\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df[numerical_columns] = knn_imputer.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# Quarterly Rating Increase\n",
        "df['Quarterly_Rating_Change'] = df.groupby('Driver_ID')['Quarterly Rating'].diff().fillna(0)\n",
        "df['Rating_Increase'] = np.where(df['Quarterly_Rating_Change'] > 0, 1, 0)\n",
        "\n",
        "# Target variable creation (1 if driver left the company, 0 otherwise)\n",
        "df['Target'] = np.where(df['Last Working Date'].notna(), 1, 0)\n",
        "\n",
        "# Income Increase\n",
        "df['Income_Change'] = df.groupby('Driver_ID')['Income'].diff().fillna(0)\n",
        "df['Income_Increase'] = np.where(df['Income_Change'] > 0, 1, 0)\n",
        "\n",
        "# Tenure calculation (difference in days)\n",
        "df['Tenure'] = (df['Last Working Date'] - df['Date of Joining']).dt.days\n",
        "df['Tenure'].fillna((df['Reporting Date'] - df['Date of Joining']).dt.days, inplace=True)\n",
        "\n",
        "# Business Value Per Day\n",
        "df['Business_Value_Per_Day'] = df['Total Business Value'] / df['Reporting Date'].dt.days_in_month\n",
        "\n",
        "# Step 4: Class Imbalance Treatment using SMOTE\n",
        "# First, split data into features and target\n",
        "X = df.drop(columns=['Target', 'Last Working Date', 'Date of Joining', 'Reporting Date', 'Driver_ID'])\n",
        "y = df['Target']\n",
        "\n",
        "# Encoding categorical variables using One-Hot Encoding\n",
        "X = pd.get_dummies(X, columns=['Gender', 'City', 'Education_Level', 'Joining Designation', 'Grade'], drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 5: Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Model Building using RandomForest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train_smote)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "y_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Step 7: Evaluation\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ROC AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "\n",
        "# Plotting ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='orange', label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gc0eMvTIPwbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column indicating whether the Quarterly Rating has increased\n",
        "df['Quarterly_Rating_Change'] = df.groupby('Driver_ID')['Quarterly Rating'].diff().fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Create a column indicating whether the Monthly Income has increased\n",
        "df['Income_Change'] = df.groupby('Driver_ID')['Income'].diff().fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Create target variable (1 if driver has left, 0 otherwise)\n",
        "df['Target'] = df['Last Working Date'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
        "\n",
        "# Aggregating the new features by Driver_ID\n",
        "df_features = df.groupby('Driver_ID').agg({\n",
        "    'Quarterly_Rating_Change': 'max',\n",
        "    'Income_Change': 'max',\n",
        "    'Target': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the new features with the aggregated data\n",
        "df_agg = df_agg.merge(df_features, on='Driver_ID', how='left')\n",
        "\n",
        "# Check the final aggregated DataFrame\n",
        "print(df_agg.head())\n"
      ],
      "metadata": {
        "id": "ARf2beyuPzQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Analysis\n",
        "sns.histplot(df_agg['Age'], kde=True)\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(x='Gender', data=df_agg)\n",
        "plt.show()\n",
        "\n",
        "# Bivariate Analysis\n",
        "if 'Target' not in df_agg.columns:\n",
        "    df_features = df.groupby('Driver_ID').agg({\n",
        "        'Quarterly_Rating_Change': 'max',\n",
        "        'Income_Change': 'max',\n",
        "        'Target': 'max'\n",
        "    }).reset_index()\n",
        "    df_agg = df_agg.merge(df_features[['Driver_ID', 'Target']], on='Driver_ID', how='left')\n",
        "\n",
        "sns.scatterplot(x='Age', y='Income', hue='Target', data=df_agg)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gxbI7hftQmxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate correlation matrix\n",
        "plt.figure(figsize=(12,8))\n",
        "# Selecting only numerical features for correlation analysis\n",
        "numerical_data = df_agg.select_dtypes(include=np.number)\n",
        "corr_matrix = numerical_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_EtZ_Ct2QFAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding for categorical variables\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "encoded_features = encoder.fit_transform(df_agg[['City', 'Education_Level']])\n",
        "\n",
        "# Standardization of numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df_agg[['Age', 'Income', 'Total Business Value', 'Grade']])\n",
        "\n",
        "# Combine scaled numerical features and encoded categorical features\n",
        "X = np.hstack((scaled_features, encoded_features))\n",
        "\n",
        "# Addressing class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, df_agg['Target'])"
      ],
      "metadata": {
        "id": "I8IKBZWvQFa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging - Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Boosting - Gradient Boosting\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "# Hyperparameter tuning (example with Random Forest)\n",
        "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "A5MjDlwYRYHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print(\"Gradient Boosting Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "\n",
        "# ROC AUC Score and Curve\n",
        "rf_auc = roc_auc_score(y_test, y_pred_rf)\n",
        "gb_auc = roc_auc_score(y_test, y_pred_gb)\n",
        "\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_pred_gb)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {rf_auc:.2f})\")\n",
        "plt.plot(fpr_gb, tpr_gb, label=f\"Gradient Boosting (AUC = {gb_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SqYb6G3CRl-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QB0Yyz_hRvKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROC Curve Analysis:**\n",
        "\n",
        "**Random Forest:**   \n",
        "AUC = 0.78\n",
        "\n",
        "* This indicates a better overall performance compared to Gradient Boosting.\n",
        "\n",
        "* The curve is closer to the top-left corner, meaning a better trade-off between true positive and false positive rates.\n",
        "\n",
        "**Gradient Boosting:**\n",
        "\n",
        "AUC = 0.73\n",
        "\n",
        "\n",
        "* Slightly lower AUC compared to Random Forest.\n",
        "\n",
        "* The curve is less optimal, indicating a poorer trade-off compared to Random Forest.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8JSSWRy5R4nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Report Summary**:\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "* Precision: 0.79 for class 0, 0.77 for class 1.\n",
        "\n",
        "* Recall: 0.76 for class 0, 0.79 for class 1.\n",
        "\n",
        "* F1-Score: 0.78 for both classes.\n",
        "\n",
        "**Gradient Boosting:**\n",
        "\n",
        "* Precision: 0.75 for class 0, 0.71 for class 1.\n",
        "\n",
        "* Recall: 0.68 for class 0, 0.77 for class 1.\n",
        "\n",
        "* F1-Score: 0.72 for class 0, 0.74 for class 1.\n"
      ],
      "metadata": {
        "id": "pCj7R3D6SIXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Actionable Insights & Recommendations**\n",
        "1. Model Performance Comparison\n",
        "* Random Forest outperforms Gradient Boosting in this task, as indicated by its higher AUC (0.78 vs. 0.73). The Random Forest model shows a better trade-off between true positive and false positive rates, making it a more reliable model for predicting driver attrition.\n",
        "\n",
        "* Precision and recall metrics suggest that Random Forest is slightly more balanced in handling both classes (drivers who stay and those who leave) compared to Gradient Boosting. The F1-Score is equal for both classes in Random Forest, showing consistency in performance.\n",
        "2. Model Selection Recommendation\n",
        "\n",
        "* Given the performance metrics, Random Forest should be the primary model used for predicting driver attrition. It provides a better balance between correctly identifying drivers who will leave (class 1) and those who will stay (class 0).\n",
        "3. Targeted Retention Strategies\n",
        "\n",
        "* **Identify at-risk drivers:** Utilize the predictions from the Random Forest model to identify drivers at high risk of leaving. Focus retention strategies on these drivers.\n",
        "* **Personalized incentives:** For drivers predicted to leave, consider personalized incentives such as bonuses, flexible working hours, or tailored engagement programs to improve their satisfaction and reduce attrition.\n",
        "4. Enhancing Model Features\n",
        "\n",
        "* **Quarterly Rating Improvement:** Since the quarterly rating is an important feature, focus on interventions that can improve driver ratings. For example, providing additional training or resources to help drivers perform better could lead to higher retention rates.\n",
        "* **Income Monitoring:** Continuously monitor the income trends of drivers. Drivers with declining income might be more likely to leave. Creating programs to stabilize or increase income for these drivers could reduce attrition.\n",
        "5. Operational Insights\n",
        "\n",
        "*  **City-Specific Retention Programs:** If the analysis shows city-specific differences in attrition rates, tailor retention programs to address the unique challenges faced by drivers in those cities.\n",
        "\n",
        "* **Demographic Tailoring:** Analyze demographic factors like age and education level to identify segments that might be more prone to leaving. Tailor engagement and retention strategies accordingly.\n",
        "6. Further Model Improvements\n",
        "* **Hyperparameter Tuning:** While Random Forest is performing well, there might be further room for improvement through hyperparameter tuning. This can potentially enhance model accuracy and the ability to generalize to unseen data.\n",
        "* **Ensemble Methods:** Explore stacking or other advanced ensemble methods that combine Random Forest and Gradient Boosting, potentially leveraging the strengths of both models.\n",
        "7. Driver Engagement\n",
        "* **Feedback Loop:** Implement a feedback mechanism where drivers can voice their concerns or suggestions. This data can be used to enhance the model and improve retention strategies.\n",
        "\n",
        "* **Regular Monitoring:** Establish a regular monitoring system using the model predictions to track and address attrition risks before they materialize."
      ],
      "metadata": {
        "id": "3UTjbvyMSh79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save feature names after preprocessing\n",
        "feature_columns = df_agg.columns.tolist()  # Use X from your training data\n",
        "\n",
        "# Create input widgets\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from datetime import date\n",
        "\n",
        "# Widgets for features with city names\n",
        "city_mapping = [\n",
        "    ('Pune', 'C23'),\n",
        "    ('Mumbai', 'C7'),\n",
        "    ('Varanasi', 'C13'),\n",
        "    ('Goa', 'C9'),\n",
        "    ('Dubai', 'C11')\n",
        "]\n",
        "\n",
        "age = widgets.FloatText(description=\"Age:\", value=30)\n",
        "gender = widgets.Dropdown(options=[('Female', 0), ('Male', 1)], description=\"Gender:\", value=0)\n",
        "city = widgets.Dropdown(options=city_mapping, description=\"City:\", value='C23')\n",
        "education = widgets.Dropdown(options=[('High School', 0), ('Bachelor', 1), ('Master', 2)],\n",
        "                           description=\"Education Level:\", value=1)\n",
        "income = widgets.FloatText(description=\"Income:\", value=50000)\n",
        "joining_designation = widgets.Dropdown(options=[('Junior', 1), ('Mid', 2), ('Senior', 3)],\n",
        "                                     description=\"Joining Designation:\", value=2)\n",
        "grade = widgets.Dropdown(options=[('I', 1), ('II', 2), ('III', 3)], description=\"Grade:\", value=2)\n",
        "total_business_value = widgets.FloatText(description=\"Business Value:\", value=100000)\n",
        "quarterly_rating = widgets.FloatText(description=\"Quarterly Rating:\", value=3.0)\n",
        "date_of_joining = widgets.DatePicker(description=\"Joining Date:\", value=date(2020, 1, 1))\n",
        "reporting_date = widgets.DatePicker(description=\"Reporting Date:\", value=date(2023, 1, 1))\n",
        "\n",
        "# Prediction button and output\n",
        "button = widgets.Button(description=\"Predict\", button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.VBox([\n",
        "    widgets.HBox([age, gender]),\n",
        "    widgets.HBox([city, education]),\n",
        "    widgets.HBox([income, total_business_value]),\n",
        "    widgets.HBox([joining_designation, grade]),\n",
        "    quarterly_rating,\n",
        "    widgets.HBox([date_of_joining, reporting_date]),\n",
        "    button,\n",
        "    output\n",
        "]))\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        try:\n",
        "            # Calculate derived features\n",
        "            tenure = (reporting_date.value - date_of_joining.value).days\n",
        "            business_value_per_day = total_business_value.value / reporting_date.value.day\n",
        "\n",
        "            # Create input DataFrame\n",
        "            input_data = pd.DataFrame([{\n",
        "                'Age': age.value,\n",
        "                'Gender': gender.value,\n",
        "                'City': city.value,\n",
        "                'Education_Level': education.value,\n",
        "                'Income': income.value,\n",
        "                'Joining Designation': joining_designation.value,\n",
        "                'Grade': grade.value,\n",
        "                'Total Business Value': total_business_value.value,\n",
        "                'Quarterly Rating': quarterly_rating.value,\n",
        "                'Tenure': tenure,\n",
        "                'Business_Value_Per_Day': business_value_per_day,\n",
        "                'Quarterly_Rating_Change': 0,  # Default value\n",
        "                'Income_Change': 0             # Default value\n",
        "            }])\n",
        "\n",
        "            # Preprocess input\n",
        "            input_scaled = preprocess_input(input_data, feature_columns, scaler)\n",
        "\n",
        "            # Make prediction\n",
        "            proba = rf_model.predict_proba(input_scaled)[0][1]\n",
        "            prediction = \"High Risk\" if proba >= 0.5 else \"Low Risk\"\n",
        "\n",
        "            # Display results\n",
        "            print(\"🚨 Driver Attrition Prediction Report 🚨\")\n",
        "            print(\"=\"*45)\n",
        "            print(f\"Prediction: {prediction} ({proba*100:.1f}% probability)\")\n",
        "            print(\"\\n🔍 Key Risk Factors:\")\n",
        "            print(f\"- City: {dict(city_mapping)[city.value]} (Code: {city.value})\")\n",
        "            print(f\"- Quarterly Rating: {quarterly_rating.value}/5\")\n",
        "            print(f\"- Tenure: {tenure} days ({tenure//365} years)\")\n",
        "            print(f\"- Income: ₹{income.value:,.0f}\")\n",
        "            print(f\"- Education Level: {dict(education.options)[education.value]}\")\n",
        "            print(f\"- Business Value/Day: ₹{business_value_per_day:,.0f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {str(e)}\")\n",
        "\n",
        "# Connect button to function\n",
        "button.on_click(on_button_click)"
      ],
      "metadata": {
        "id": "6tEt2BSKZDrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9PaWzNdnZ498"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}